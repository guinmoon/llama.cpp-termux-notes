# Build and run llama.cpp in Termux
